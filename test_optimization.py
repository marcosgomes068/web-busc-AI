"""
Teste das Otimiza√ß√µes do Sistema
================================

Script para testar as melhorias de performance e qualidade dos prompts.

Autor: Marco
Data: Agosto 2025
"""

import time
import json
from core.co import create_agent, test_api_connection
from core.config import SystemConfig
from utils.console import (
    print_header, print_section, print_step, print_result, 
    log, RichProgress, RichTable, console
)


def test_optimized_prompts():
    """Testa os prompts otimizados com diferentes tamanhos de conte√∫do."""
    print_section("TESTANDO PROMPTS OTIMIZADOS")
    
    # Verifica conex√£o
    if not test_api_connection():
        log.error("Falha na conex√£o com API. Configure COHERE_API_KEY")
        return
    
    # Conte√∫do de teste
    test_contents = {
        'pequeno': "Python √© uma linguagem de programa√ß√£o vers√°til e f√°cil de aprender.",
        'medio': """
Python √© uma linguagem de programa√ß√£o de alto n√≠vel, interpretada e de prop√≥sito geral.
Foi criada por Guido van Rossum e lan√ßada pela primeira vez em 1991.
Python enfatiza a legibilidade do c√≥digo e sua sintaxe permite que os programadores
expressem conceitos em menos linhas de c√≥digo do que seria poss√≠vel em linguagens
como C++ ou Java. A linguagem fornece constru√ß√µes que permitem programa√ß√£o clara
em pequena e grande escala.
        """,
        'grande': """
Python √© uma linguagem de programa√ß√£o de alto n√≠vel, interpretada, de script, 
imperativa, orientada a objetos, funcional, de tipagem din√¢mica e forte.
Foi lan√ßada por Guido van Rossum em 1991. Atualmente possui um modelo de 
desenvolvimento comunit√°rio, aberto e gerenciado pela organiza√ß√£o sem fins 
lucrativos Python Software Foundation. Apesar de v√°rias partes da linguagem 
possu√≠rem padr√µes e especifica√ß√µes formais, a linguagem como um todo n√£o √© 
formalmente especificada. O padr√£o de facto √© a implementa√ß√£o CPython.
A filosofia de design do Python enfatiza a import√¢ncia do esfor√ßo do programador 
sobre o esfor√ßo computacional. Prioriza a legibilidade do c√≥digo sobre a velocidade 
ou expressividade. Combina uma sintaxe concisa e clara com os recursos poderosos 
de sua biblioteca padr√£o e por uma vasta gama de m√≥dulos e frameworks 
desenvolvidos por terceiros. Python √© uma linguagem de prop√≥sito geral de alto 
n√≠vel, multiparadigma, suporta o paradigma orientado a objetos, imperativo, 
funcional e procedural. Possui tipagem din√¢mica e uma de suas principais 
caracter√≠sticas √© permitir a f√°cil leitura do c√≥digo e exigir poucas linhas 
de c√≥digo se comparado ao mesmo programa em outras linguagens.
        """ * 2
    }
    
    # Cria agente otimizado
    log.step("Criando agente com prompts otimizados")
    optimized_agent = create_agent(
        "ESPECIALISTA EM AN√ÅLISE T√âCNICA\n"
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"
        "MISS√ÉO: Analisar conte√∫do t√©cnico e extrair insights importantes.\n\n"
        "INSTRU√á√ïES:\n"
        "‚Ä¢ Identifique pontos-chave\n"
        "‚Ä¢ Use linguagem t√©cnica apropriada\n"
        "‚Ä¢ Seja conciso mas informativo\n"
        "‚Ä¢ Foque em aspectos pr√°ticos\n\n"
        "FORMATO: Resposta estruturada em portugu√™s brasileiro.",
        max_tokens=400
    )
    
    # Testa com diferentes tamanhos
    test_results = []
    
    with RichProgress("Testando conte√∫dos de diferentes tamanhos") as progress:
        total_tests = len(test_contents)
        for i, (size_name, content) in enumerate(test_contents.items()):
            progress.update((i / total_tests) * 100, f"Testando conte√∫do {size_name}")
            
            start_time = time.time()
            
            try:
                response = optimized_agent(f"Analise este conte√∫do sobre Python:\n\n{content}")
                processing_time = time.time() - start_time
                
                test_results.append({
                    "Tamanho": size_name.title(),
                    "Chars Input": len(content),
                    "Chars Output": len(response),
                    "Tempo (s)": f"{processing_time:.2f}",
                    "Status": "‚úÖ Sucesso"
                })
                
                log.success(f"Conte√∫do {size_name} processado em {processing_time:.2f}s")
                
            except Exception as e:
                test_results.append({
                    "Tamanho": size_name.title(),
                    "Chars Input": len(content),
                    "Chars Output": 0,
                    "Tempo (s)": "N/A",
                    "Status": f"‚ùå {str(e)[:30]}..."
                })
                log.error(f"Erro no conte√∫do {size_name}: {e}")
    
    # Exibe tabela de resultados
    if test_results:
        table = RichTable.create_summary_table(test_results, "Resultados dos Testes de Prompts")
        console.print(table)
    
    log.success("Teste de prompts otimizados conclu√≠do!")


def test_system_config():
    """Testa as configura√ß√µes do sistema."""
    print_section("TESTANDO CONFIGURA√á√ïES DO SISTEMA")
    
    # Testa configura√ß√µes de diferentes tamanhos
    sizes = [500, 1500, 4000]
    config_results = []
    
    for size in sizes:
        config = SystemConfig.get_cohere_config(size)
        config_results.append({
            "Tamanho": f"{size} chars",
            "Tokens": config['max_tokens'],
            "Temperature": config['temperature'],
            "P-value": config['p']
        })
    
    # Exibe tabela de configura√ß√µes por tamanho
    if config_results:
        table = RichTable.create_summary_table(config_results, "Configura√ß√µes por Tamanho de Conte√∫do")
        console.print(table)
    
    # Testa configura√ß√µes de agentes
    agents = ['resumidor', 'analista', 'organizador', 'sintetizador']
    agent_results = []
    
    for agent in agents:
        config = SystemConfig.get_agent_config(agent)
        agent_results.append({
            "Agente": agent.title(),
            "Tokens": config['max_tokens'],
            "Temperature": config.get('temperature', 'N/A'),
            "Especializa√ß√£o": config.get('role', 'N/A')[:30] + "..."
        })
    
    # Exibe tabela de configura√ß√µes de agentes
    if agent_results:
        table = RichTable.create_summary_table(agent_results, "Configura√ß√µes dos Agentes Especializados")
        console.print(table)
    
    log.success("Teste de configura√ß√µes conclu√≠do!")


def benchmark_performance():
    """Benchmark b√°sico de performance."""
    print_section("BENCHMARK DE PERFORMANCE")
    
    if not test_api_connection():
        log.error("N√£o √© poss√≠vel fazer benchmark sem conex√£o API")
        return
    
    # Cria agente simples para teste
    test_agent = create_agent("Responda de forma concisa sobre o tema apresentado.")
    
    test_prompts = [
        "O que √© Python?",
        "Explique programa√ß√£o orientada a objetos em Python.",
        "Descreva as vantagens do Python para ci√™ncia de dados.",
    ]
    
    benchmark_results = []
    total_time = 0
    successful_calls = 0
    
    with RichProgress("Executando benchmark de performance") as progress:
        for i, prompt in enumerate(test_prompts):
            progress.update((i / len(test_prompts)) * 100, f"Teste {i+1}/{len(test_prompts)}")
            
            start_time = time.time()
            
            try:
                response = test_agent(prompt)
                end_time = time.time()
                
                call_time = end_time - start_time
                total_time += call_time
                successful_calls += 1
                
                benchmark_results.append({
                    "Teste": f"#{i+1}",
                    "Prompt": prompt[:30] + "...",
                    "Tempo (s)": f"{call_time:.2f}",
                    "Chars": len(response),
                    "Status": "‚úÖ Sucesso"
                })
                
                log.success(f"Teste {i+1} conclu√≠do em {call_time:.2f}s")
                
            except Exception as e:
                benchmark_results.append({
                    "Teste": f"#{i+1}",
                    "Prompt": prompt[:30] + "...",
                    "Tempo (s)": "N/A",
                    "Chars": 0,
                    "Status": f"‚ùå {str(e)[:20]}..."
                })
                log.error(f"Falha no teste {i+1}: {e}")
    
    # Exibe resultados do benchmark
    if benchmark_results:
        table = RichTable.create_summary_table(benchmark_results, "Resultados do Benchmark")
        console.print(table)
    
    if successful_calls > 0:
        avg_time = total_time / successful_calls
        console.print(f"\n[bold green]üìä ESTAT√çSTICAS FINAIS:[/bold green]")
        console.print(f"   [cyan]‚Ä¢[/cyan] Chamadas bem-sucedidas: [bold]{successful_calls}/{len(test_prompts)}[/bold]")
        console.print(f"   [cyan]‚Ä¢[/cyan] Tempo m√©dio por chamada: [bold]{avg_time:.2f}s[/bold]")
        console.print(f"   [cyan]‚Ä¢[/cyan] Tempo total: [bold]{total_time:.2f}s[/bold]")
    else:
        log.error("Nenhuma chamada bem-sucedida")


if __name__ == "__main__":
    print_header("SISTEMA DE TESTES DE OTIMIZA√á√ÉO", "Valida√ß√£o completa das melhorias implementadas")
    
    try:
        # Executa todos os testes
        test_system_config()
        test_optimized_prompts()
        benchmark_performance()
        
        console.print()
        log.success("TODOS OS TESTES CONCLU√çDOS!")
        log.info("Sistema otimizado e pronto para uso.")
        
    except Exception as e:
        log.error(f"Erro durante testes: {e}")
        log.info("Verifique a configura√ß√£o da API Cohere.")
