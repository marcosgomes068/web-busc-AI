"""
MÃ³dulo de Coleta de ConteÃºdo Web
================================

Este mÃ³dulo gerencia a coleta e extraÃ§Ã£o de conteÃºdo de pÃ¡ginas web,
incluindo busca de URLs, extraÃ§Ã£o de texto e processamento de dados.

Funcionalidades:
- Busca inteligente de URLs baseada em termos
- ExtraÃ§Ã£o otimizada de conteÃºdo textual
- Limpeza e normalizaÃ§Ã£o de texto
- Salvamento estruturado em JSON
- Tratamento robusto de erros

DependÃªncias:
- requests: Para requisiÃ§Ãµes HTTP
- beautifulsoup4: Para parsing de HTML
- json: Para serializaÃ§Ã£o de dados
- datetime: Para timestamps

Autor: Marco
Data: Agosto 2025
"""

import requests
from utils.console import (
    print_header, print_section, print_step, print_result, 
    log, RichProgress, RichStatus, console
)
from bs4 import BeautifulSoup
import urllib.parse
import json
import os
import re
from datetime import datetime
from typing import List, Dict, Any, Optional


# ConfiguraÃ§Ãµes globais
REQUEST_TIMEOUT = 15
MAX_CONTENT_LENGTH = 8000  # Aumentado para capturar mais conteÃºdo
DEFAULT_USER_AGENT = (
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
    '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
)

# Base de URLs educacionais para Python
PYTHON_RESOURCES = {
    "aprender python": [
        "https://docs.python.org/pt-br/3/tutorial/",
        "https://www.python.org/",
        "https://realpython.com/",
        "https://www.w3schools.com/python/",
        "https://pythonacademy.com.br/"
    ],
    "tutorial python": [
        "https://docs.python.org/pt-br/3/tutorial/",
        "https://www.codecademy.com/learn/learn-python-3",
        "https://www.learnpython.org/",
        "https://www.tutorialspoint.com/python/",
        "https://automatetheboringstuff.com/"
    ],
    "python para iniciantes": [
        "https://www.python.org/about/gettingstarted/",
        "https://realpython.com/python-beginner-tips/",
        "https://www.w3schools.com/python/python_intro.asp",
        "https://docs.python.org/pt-br/3/tutorial/introduction.html",
        "https://pythonspot.com/"
    ],
    "sintaxe python": [
        "https://docs.python.org/pt-br/3/reference/",
        "https://www.w3schools.com/python/python_syntax.asp",
        "https://realpython.com/python-syntax/",
        "https://docs.python.org/pt-br/3/tutorial/introduction.html",
        "https://www.programiz.com/python-programming/syntax"
    ],
    "exemplos de cÃ³digo python": [
        "https://github.com/python/cpython",
        "https://realpython.com/python-practice-problems/",
        "https://www.programiz.com/python-programming/examples",
        "https://docs.python.org/pt-br/3/tutorial/",
        "https://www.w3resource.com/python-exercises/"
    ]
}


def search_pages_for_term(search_term: str, max_pages: int = 5) -> List[str]:
    """
    Busca URLs relevantes para um termo especÃ­fico.
    
    Args:
        search_term (str): Termo de busca
        max_pages (int, optional): NÃºmero mÃ¡ximo de pÃ¡ginas. Default: 5
    
    Returns:
        List[str]: Lista de URLs encontradas
        
    Note:
        Usa uma base de conhecimento prÃ©-definida para garantir
        qualidade e relevÃ¢ncia dos recursos educacionais.
    """
    # Normaliza o termo para busca case-insensitive
    normalized_term = search_term.lower().strip()
    
    # Busca correspondÃªncias na base de conhecimento
    found_urls = []
    
    for resource_key, urls in PYTHON_RESOURCES.items():
        # Verifica se alguma palavra do termo coincide com a chave
        if any(word in normalized_term for word in resource_key.split()):
            found_urls = urls[:max_pages]
            break
    
    # Fallback: URLs gerais se nÃ£o encontrar correspondÃªncia especÃ­fica
    if not found_urls:
        found_urls = [
            "https://docs.python.org/pt-br/3/",
            "https://www.python.org/",
            "https://realpython.com/",
            "https://www.w3schools.com/python/",
            "https://www.programiz.com/python-programming/"
        ][:max_pages]
    
    return found_urls


def collect_web_pages(search_terms: List[str], max_pages: int = 5) -> Dict[str, List[str]]:
    """
    Coleta pÃ¡ginas web para uma lista de termos de busca.
    
    Args:
        search_terms (List[str]): Lista de termos de busca
        max_pages (int, optional): NÃºmero mÃ¡ximo de pÃ¡ginas por termo. Default: 5
    
    Returns:
        Dict[str, List[str]]: DicionÃ¡rio mapeando termos para suas URLs
        
    Examples:
        >>> terms = ["python bÃ¡sico", "python avanÃ§ado"]
        >>> results = collect_web_pages(terms, max_pages=3)
        >>> print(results)
        {'python bÃ¡sico': ['url1', 'url2', 'url3'], ...}
    """
    search_results = {}
    
    for index, term in enumerate(search_terms, 1):
        print(f"\nğŸ” Buscando pÃ¡ginas para termo {index}: {term}")
        
        # Busca URLs para o termo atual
        found_pages = search_pages_for_term(term, max_pages)
        search_results[term] = found_pages
        
        # Mostra resultados
        print(f"ğŸ“‹ Encontradas {len(found_pages)} pÃ¡ginas:")
        for page_num, url in enumerate(found_pages, 1):
            print(f"   {page_num}. {url}")
    
    return search_results


def extract_page_content(url: str) -> Dict[str, Any]:
    """
    Extrai o conteÃºdo textual de uma pÃ¡gina web.
    
    Args:
        url (str): URL da pÃ¡gina para extrair conteÃºdo
    
    Returns:
        Dict[str, Any]: DicionÃ¡rio com dados extraÃ­dos da pÃ¡gina
        
    Structure:
        - url: URL original
        - titulo: TÃ­tulo da pÃ¡gina
        - descricao: Meta description
        - conteudo_texto: Texto limpo extraÃ­do
        - tamanho_texto: Tamanho do texto em caracteres
        - status: Status da extraÃ§Ã£o
        - timestamp: Timestamp da extraÃ§Ã£o
    """
    try:
        # ConfiguraÃ§Ã£o da requisiÃ§Ã£o
        headers = {'User-Agent': DEFAULT_USER_AGENT}
        
        # RequisiÃ§Ã£o HTTP com timeout
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        
        # Parse do HTML
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Remove elementos desnecessÃ¡rios
        _remove_unwanted_elements(soup)
        
        # Extrai metadados
        title = _extract_title(soup)
        description = _extract_meta_description(soup)
        
        # Extrai conteÃºdo principal
        main_content = _extract_main_content(soup)
        
        # Limpa e normaliza o texto
        clean_text = _clean_text_content(main_content)
        
        return {
            "url": url,
            "titulo": title,
            "descricao": description,
            "conteudo_texto": clean_text[:MAX_CONTENT_LENGTH],
            "tamanho_texto": len(clean_text),
            "status": "sucesso",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as error:
        return {
            "url": url,
            "titulo": "",
            "descricao": "",
            "conteudo_texto": "",
            "tamanho_texto": 0,
            "status": f"erro: {str(error)}",
            "timestamp": datetime.now().isoformat()
        }


def _remove_unwanted_elements(soup: BeautifulSoup) -> None:
    """Remove elementos HTML desnecessÃ¡rios para extraÃ§Ã£o de texto."""
    unwanted_tags = [
        "script", "style", "nav", "header", "footer", 
        "aside", "noscript", "iframe", "form", "button"
    ]
    
    for tag_name in unwanted_tags:
        for element in soup.find_all(tag_name):
            element.decompose()


def _extract_title(soup: BeautifulSoup) -> str:
    """Extrai o tÃ­tulo da pÃ¡gina."""
    title_element = soup.find("title")
    return title_element.get_text().strip() if title_element else "Sem tÃ­tulo"


def _extract_meta_description(soup: BeautifulSoup) -> str:
    """Extrai a meta description da pÃ¡gina."""
    meta_element = soup.find("meta", attrs={"name": "description"})
    return meta_element.get("content", "").strip() if meta_element else ""


def _extract_main_content(soup: BeautifulSoup) -> str:
    """Extrai o conteÃºdo principal da pÃ¡gina usando seletores prioritÃ¡rios."""
    # Seletores ordenados por prioridade
    priority_selectors = [
        'main', 'article', '.content', '.main-content',
        '#content', '.post-content', '.entry-content'
    ]
    
    # Tenta encontrar conteÃºdo usando seletores prioritÃ¡rios
    for selector in priority_selectors:
        elements = soup.select(selector)
        if elements:
            return elements[0].get_text()
    
    # Fallback: usa body ou todo o documento
    body_element = soup.find("body")
    if body_element:
        return body_element.get_text()
    
    return soup.get_text()


def _clean_text_content(raw_text: str) -> str:
    """Limpa e normaliza o conteÃºdo textual."""
    # Remove quebras de linha excessivas e espaÃ§os
    lines = (line.strip() for line in raw_text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    
    # Junta chunks nÃ£o vazios com tamanho mÃ­nimo
    clean_text = ' '.join(chunk for chunk in chunks if chunk and len(chunk) > 3)
    
    # Normaliza espaÃ§os em branco
    clean_text = re.sub(r'\s+', ' ', clean_text)
    
    # Remove caracteres especiais mantendo pontuaÃ§Ã£o bÃ¡sica
    clean_text = re.sub(
        r'[^\w\s\.,!?;:()\-\[\]"Ã¡Ã Ã¢Ã£Ã©Ã¨ÃªÃ­Ã¬Ã®Ã³Ã²Ã´ÃµÃºÃ¹Ã»Ã§]', 
        ' ', 
        clean_text
    )
    
    # NormalizaÃ§Ã£o final
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    return clean_text


def download_and_save_content(
    search_results: Dict[str, List[str]], 
    output_file: str = "dados_coletados.json"
) -> Dict[str, Any]:
    """
    Baixa conteÃºdo de todas as URLs e salva em arquivo JSON estruturado.
    
    Args:
        search_results (Dict[str, List[str]]): Resultados da busca por termo
        output_file (str, optional): Nome do arquivo de saÃ­da. Default: "dados_coletados.json"
    
    Returns:
        Dict[str, Any]: Dados completos coletados e organizados
        
    Structure:
        - metadata: InformaÃ§Ãµes sobre a coleta
        - dados: ConteÃºdo organizado por termo de busca
    """
    # Estrutura inicial dos dados
    complete_data = {
        "metadata": {
            "data_coleta": datetime.now().isoformat(),
            "total_termos": len(search_results),
            "total_urls": sum(len(urls) for urls in search_results.values()),
            "arquivo": output_file,
            "versao": "2.0"
        },
        "dados": {}
    }
    
    print(f"\nğŸ“¥ Iniciando download do conteÃºdo das pÃ¡ginas...")
    
    # Processa cada termo e suas URLs
    for term, urls in search_results.items():
        print(f"\nğŸ”„ Processando termo: {term}")
        complete_data["dados"][term] = []
        
        for index, url in enumerate(urls, 1):
            print(f"   ğŸ“– Baixando {index}/{len(urls)}: {url[:80]}...")
            
            # Extrai conteÃºdo da pÃ¡gina
            page_content = extract_page_content(url)
            complete_data["dados"][term].append(page_content)
    
    # Salva dados no arquivo JSON
    _save_json_data(complete_data, output_file)
    
    # Exibe estatÃ­sticas finais
    _display_collection_stats(complete_data)
    
    return complete_data


def _save_json_data(data: Dict[str, Any], filename: str) -> None:
    """Salva dados em arquivo JSON com tratamento de erros."""
    try:
        with open(filename, 'w', encoding='utf-8') as file:
            json.dump(data, file, ensure_ascii=False, indent=2)
        print(f"\nâœ… Dados salvos com sucesso em: {filename}")
        
    except Exception as error:
        print(f"âŒ Erro ao salvar arquivo JSON: {error}")
        raise


def _display_collection_stats(data: Dict[str, Any]) -> None:
    """Exibe estatÃ­sticas da coleta de dados."""
    # Calcula estatÃ­sticas
    success_count = sum(
        len([page for page in pages if page["status"] == "sucesso"]) 
        for pages in data["dados"].values()
    )
    
    error_count = sum(
        len([page for page in pages if page["status"] != "sucesso"]) 
        for pages in data["dados"].values()
    )
    
    total_pages = success_count + error_count
    
    # Exibe relatÃ³rio
    print(f"\nğŸ“Š EstatÃ­sticas da Coleta:")
    print(f"   ğŸ“„ Total de pÃ¡ginas processadas: {total_pages}")
    print(f"   âœ… Sucessos: {success_count}")
    print(f"   âŒ Erros: {error_count}")
    
    if total_pages > 0:
        success_rate = (success_count / total_pages) * 100
        print(f"   ğŸ“ˆ Taxa de sucesso: {success_rate:.1f}%")


# Aliases para compatibilidade com cÃ³digo existente
buscar_paginas = search_pages_for_term
coletar_todas_paginas = collect_web_pages
extrair_conteudo_pagina = extract_page_content
baixar_e_salvar_conteudo = download_and_save_content


if __name__ == "__main__":
    # Teste do mÃ³dulo quando executado diretamente
    print("ğŸ§ª Testando mÃ³dulo de coleta web...")
    
    test_terms = ["python bÃ¡sico", "tutorial python"]
    test_results = collect_web_pages(test_terms, max_pages=2)
    
    print(f"\nğŸ“‹ Teste concluÃ­do com {len(test_results)} termos processados.")
