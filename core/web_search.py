"""
M√≥dulo de Coleta de Conte√∫do Web
================================

Este m√≥dulo gerencia a coleta e extra√ß√£o de conte√∫do de p√°ginas web,
incluindo busca de URLs, extra√ß√£o de texto e processamento de dados.

Funcionalidades:
- Busca inteligente de URLs baseada em termos
- Extra√ß√£o otimizada de conte√∫do textual
- Limpeza e normaliza√ß√£o de texto
- Salvamento estruturado em JSON
- Tratamento robusto de erros

Depend√™ncias:
- requests: Para requisi√ß√µes HTTP
- beautifulsoup4: Para parsing de HTML
- json: Para serializa√ß√£o de dados
- datetime: Para timestamps

Autor: Marco
Data: Agosto 2025
"""

import requests
from utils.console import (
    print_header, print_section, print_step, print_result, 
    log, RichProgress, RichStatus, console
)
from bs4 import BeautifulSoup
import urllib.parse
import json
import os
import re
from datetime import datetime
from typing import List, Dict, Any, Optional


# Configura√ß√µes globais
REQUEST_TIMEOUT = 15
MAX_CONTENT_LENGTH = 8000  # Aumentado para capturar mais conte√∫do
DEFAULT_USER_AGENT = (
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
    '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
)

# Base de URLs educacionais para Python
PYTHON_RESOURCES = {
    "aprender python": [
        "https://docs.python.org/pt-br/3/tutorial/",
        "https://www.python.org/",
        "https://realpython.com/",
        "https://www.w3schools.com/python/",
        "https://pythonacademy.com.br/"
    ],
    "tutorial python": [
        "https://docs.python.org/pt-br/3/tutorial/",
        "https://www.codecademy.com/learn/learn-python-3",
        "https://www.learnpython.org/",
        "https://www.tutorialspoint.com/python/",
        "https://automatetheboringstuff.com/"
    ],
    "python para iniciantes": [
        "https://www.python.org/about/gettingstarted/",
        "https://realpython.com/python-beginner-tips/",
        "https://www.w3schools.com/python/python_intro.asp",
        "https://docs.python.org/pt-br/3/tutorial/introduction.html",
        "https://pythonspot.com/"
    ],
    "sintaxe python": [
        "https://docs.python.org/pt-br/3/reference/",
        "https://www.w3schools.com/python/python_syntax.asp",
        "https://realpython.com/python-syntax/",
        "https://docs.python.org/pt-br/3/tutorial/introduction.html",
        "https://www.programiz.com/python-programming/syntax"
    ],
    "exemplos de c√≥digo python": [
        "https://github.com/python/cpython",
        "https://realpython.com/python-practice-problems/",
        "https://www.programiz.com/python-programming/examples",
        "https://docs.python.org/pt-br/3/tutorial/",
        "https://www.w3resource.com/python-exercises/"
    ]
}


def search_pages_for_term(search_term: str, max_pages: int = 5) -> List[str]:
    """
    Busca URLs relevantes para um termo espec√≠fico.
    
    Args:
        search_term (str): Termo de busca
        max_pages (int, optional): N√∫mero m√°ximo de p√°ginas. Default: 5
    
    Returns:
        List[str]: Lista de URLs encontradas
        
    Note:
        Usa uma base de conhecimento pr√©-definida para garantir
        qualidade e relev√¢ncia dos recursos educacionais.
    """
    # Normaliza o termo para busca case-insensitive
    normalized_term = search_term.lower().strip()
    
    # Busca correspond√™ncias na base de conhecimento
    found_urls = []
    
    for resource_key, urls in PYTHON_RESOURCES.items():
        # Verifica se alguma palavra do termo coincide com a chave
        if any(word in normalized_term for word in resource_key.split()):
            found_urls = urls[:max_pages]
            break
    
    # Fallback: URLs gerais se n√£o encontrar correspond√™ncia espec√≠fica
    if not found_urls:
        found_urls = [
            "https://docs.python.org/pt-br/3/",
            "https://www.python.org/",
            "https://realpython.com/",
            "https://www.w3schools.com/python/",
            "https://www.programiz.com/python-programming/"
        ][:max_pages]
    
    return found_urls


def collect_web_pages(search_terms: List[str], max_pages: int = 5) -> Dict[str, List[str]]:
    """
    Coleta p√°ginas web para uma lista de termos de busca.
    
    Args:
        search_terms (List[str]): Lista de termos de busca
        max_pages (int, optional): N√∫mero m√°ximo de p√°ginas por termo. Default: 5
    
    Returns:
        Dict[str, List[str]]: Dicion√°rio mapeando termos para suas URLs
        
    Examples:
        >>> terms = ["python b√°sico", "python avan√ßado"]
        >>> results = collect_web_pages(terms, max_pages=3)
        >>> print(results)
        {'python b√°sico': ['url1', 'url2', 'url3'], ...}
    """
    search_results = {}
    
    for index, term in enumerate(search_terms, 1):
        print(f"\nüîç Buscando p√°ginas para termo {index}: {term}")
        
        # Busca URLs para o termo atual
        found_pages = search_pages_for_term(term, max_pages)
        search_results[term] = found_pages
        
        # Mostra resultados
        print(f"üìã Encontradas {len(found_pages)} p√°ginas:")
        for page_num, url in enumerate(found_pages, 1):
            print(f"   {page_num}. {url}")
    
    return search_results


def extract_page_content(url: str) -> Dict[str, Any]:
    """
    Extrai o conte√∫do textual de uma p√°gina web.
    
    Args:
        url (str): URL da p√°gina para extrair conte√∫do
    
    Returns:
        Dict[str, Any]: Dicion√°rio com dados extra√≠dos da p√°gina
        
    Structure:
        - url: URL original
        - titulo: T√≠tulo da p√°gina
        - descricao: Meta description
        - conteudo_texto: Texto limpo extra√≠do
        - tamanho_texto: Tamanho do texto em caracteres
        - status: Status da extra√ß√£o
        - timestamp: Timestamp da extra√ß√£o
    """
    try:
        # Configura√ß√£o da requisi√ß√£o
        headers = {'User-Agent': DEFAULT_USER_AGENT}
        
        # Requisi√ß√£o HTTP com timeout
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        
        # Parse do HTML
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Remove elementos desnecess√°rios
        _remove_unwanted_elements(soup)
        
        # Extrai metadados
        title = _extract_title(soup)
        description = _extract_meta_description(soup)
        
        # Extrai conte√∫do principal
        main_content = _extract_main_content(soup)
        
        # Limpa e normaliza o texto
        clean_text = _clean_text_content(main_content)
        
        return {
            "url": url,
            "titulo": title,
            "descricao": description,
            "conteudo_texto": clean_text[:MAX_CONTENT_LENGTH],
            "tamanho_texto": len(clean_text),
            "status": "sucesso",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as error:
        return {
            "url": url,
            "titulo": "",
            "descricao": "",
            "conteudo_texto": "",
            "tamanho_texto": 0,
            "status": f"erro: {str(error)}",
            "timestamp": datetime.now().isoformat()
        }


def _remove_unwanted_elements(soup: BeautifulSoup) -> None:
    """Remove elementos HTML desnecess√°rios para extra√ß√£o de texto."""
    unwanted_tags = [
        "script", "style", "nav", "header", "footer", 
        "aside", "noscript", "iframe", "form", "button"
    ]
    
    for tag_name in unwanted_tags:
        for element in soup.find_all(tag_name):
            element.decompose()


def _extract_title(soup: BeautifulSoup) -> str:
    """Extrai o t√≠tulo da p√°gina."""
    title_element = soup.find("title")
    return title_element.get_text().strip() if title_element else "Sem t√≠tulo"


def _extract_meta_description(soup: BeautifulSoup) -> str:
    """Extrai a meta description da p√°gina."""
    meta_element = soup.find("meta", attrs={"name": "description"})
    return meta_element.get("content", "").strip() if meta_element else ""


def _extract_main_content(soup: BeautifulSoup) -> str:
    """Extrai o conte√∫do principal da p√°gina usando seletores priorit√°rios."""
    # Seletores ordenados por prioridade
    priority_selectors = [
        'main', 'article', '.content', '.main-content',
        '#content', '.post-content', '.entry-content'
    ]
    
    # Tenta encontrar conte√∫do usando seletores priorit√°rios
    for selector in priority_selectors:
        elements = soup.select(selector)
        if elements:
            return elements[0].get_text()
    
    # Fallback: usa body ou todo o documento
    body_element = soup.find("body")
    if body_element:
        return body_element.get_text()
    
    return soup.get_text()


def _clean_text_content(raw_text: str) -> str:
    """Limpa e normaliza o conte√∫do textual."""
    # Remove quebras de linha excessivas e espa√ßos
    lines = (line.strip() for line in raw_text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    
    # Junta chunks n√£o vazios com tamanho m√≠nimo
    clean_text = ' '.join(chunk for chunk in chunks if chunk and len(chunk) > 3)
    
    # Normaliza espa√ßos em branco
    clean_text = re.sub(r'\s+', ' ', clean_text)
    
    # Remove caracteres especiais mantendo pontua√ß√£o b√°sica
    clean_text = re.sub(
        r'[^\w\s\.,!?;:()\-\[\]"√°√†√¢√£√©√®√™√≠√¨√Æ√≥√≤√¥√µ√∫√π√ª√ß]', 
        ' ', 
        clean_text
    )
    
    # Normaliza√ß√£o final
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    return clean_text


def download_and_save_content(
    search_results: Dict[str, List[str]], 
    output_file: str = "dados_coletados.json"
) -> Dict[str, Any]:
    """
    Baixa conte√∫do de todas as URLs e salva em arquivo JSON estruturado.
    
    Args:
        search_results (Dict[str, List[str]]): Resultados da busca por termo
        output_file (str, optional): Nome do arquivo de sa√≠da. Default: "dados_coletados.json"
    
    Returns:
        Dict[str, Any]: Dados completos coletados e organizados
        
    Structure:
        - metadata: Informa√ß√µes sobre a coleta
        - dados: Conte√∫do organizado por termo de busca
    """
    # Estrutura inicial dos dados
    complete_data = {
        "metadata": {
            "data_coleta": datetime.now().isoformat(),
            "total_termos": len(search_results),
            "total_urls": sum(len(urls) for urls in search_results.values()),
            "arquivo": output_file,
            "versao": "2.0"
        },
        "dados": {}
    }
    
    print(f"\nüì• Iniciando download do conte√∫do das p√°ginas...")
    
    # Processa cada termo e suas URLs
    for term, urls in search_results.items():
        print(f"\nüîÑ Processando termo: {term}")
        complete_data["dados"][term] = []
        
        for index, url in enumerate(urls, 1):
            print(f"   üìñ Baixando {index}/{len(urls)}: {url[:80]}...")
            
            # Extrai conte√∫do da p√°gina
            page_content = extract_page_content(url)
            complete_data["dados"][term].append(page_content)
    
    # Salva dados no arquivo JSON
    _save_json_data(complete_data, output_file)
    
    # Exibe estat√≠sticas finais
    _display_collection_stats(complete_data)
    
    return complete_data


def _save_json_data(data: Dict[str, Any], filename: str) -> None:
    """Salva dados em arquivo JSON com tratamento de erros."""
    try:
        with open(filename, 'w', encoding='utf-8') as file:
            json.dump(data, file, ensure_ascii=False, indent=2)
        print(f"\n‚úÖ Dados salvos com sucesso em: {filename}")
        
    except Exception as error:
        print(f"‚ùå Erro ao salvar arquivo JSON: {error}")
        raise


def _display_collection_stats(data: Dict[str, Any]) -> None:
    """Exibe estat√≠sticas da coleta de dados."""
    # Calcula estat√≠sticas
    success_count = sum(
        len([page for page in pages if page["status"] == "sucesso"]) 
        for pages in data["dados"].values()
    )
    
    error_count = sum(
        len([page for page in pages if page["status"] != "sucesso"]) 
        for pages in data["dados"].values()
    )
    
    total_pages = success_count + error_count
    
    # Exibe relat√≥rio
    print(f"\nüìä Estat√≠sticas da Coleta:")
    print(f"   üìÑ Total de p√°ginas processadas: {total_pages}")
    print(f"   ‚úÖ Sucessos: {success_count}")
    print(f"   ‚ùå Erros: {error_count}")
    
    if total_pages > 0:
        success_rate = (success_count / total_pages) * 100
        print(f"   üìà Taxa de sucesso: {success_rate:.1f}%")


# Aliases para compatibilidade com c√≥digo existente
buscar_paginas = search_pages_for_term
coletar_todas_paginas = collect_web_pages
extrair_conteudo_pagina = extract_page_content
baixar_e_salvar_conteudo = download_and_save_content


if __name__ == "__main__":
    # Teste do m√≥dulo quando executado diretamente
    print("üß™ Testando m√≥dulo de coleta web...")
    
    test_terms = ["python b√°sico", "tutorial python"]
    test_results = collect_web_pages(test_terms, max_pages=2)
    
    print(f"\nüìã Teste conclu√≠do com {len(test_results)} termos processados.")
